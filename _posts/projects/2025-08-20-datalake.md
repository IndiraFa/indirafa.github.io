---
layout: page-fullwidth
header:
    image_fullwidth: datalake_header_unsplash.jpg
    title: "Datalake for mobility insights"
subheadline: "Building a smart datalake for mobility insights in Île-de-France"
teaser: This project was conducted as part of the Specialized Master in Artificial Intelligence, Data Expert and MlOps at Telecom Paris. The objective was to set up an analytical and research-oriented DataLake that combines several tools for data ingestion, transformation, and visualization.
tags:
    - Datalake
    - Airflow
categories:
    - projects
---
<!--more-->

The full code is available here : [github.com](https://github.com/IndiraFa/datalake_public)

In today’s data-driven world, urban mobility operators face a critical challenge: how to leverage vast streams of real-time information to make smarter strategic decisions. For my recent project, I designed and implemented a complete analytical DataLake that transforms raw mobility data into actionable insights.
The focus of this project was the bike-sharing ecosystem in Île-de-France, with two major players in scope: Vélib, a station-based system, and Lime, a free-floating bike operator. The goal was to understand usage patterns, geographic distribution, and competitive dynamics—ultimately providing operators with a tool to identify strategic growth zones.


## From Raw Data to Business Insights

The project involved building a full data pipeline, capable of handling ingestion, transformation, modeling, and visualization. The architecture was orchestrated with Apache Airflow and deployed on AWS S3 for centralized storage.

![Datalake arcihtecture](/images/datalake/archi.png)


Main components of the workflow:
- Ingestion: APIs were queried regularly, fetching data from:
    - Vélib station_status (availability of bikes at each station).
    - Vélib station_information (static station metadata such as location).
    - Lime free_bike_status (real-time GPS position of free-floating bikes).

Data was stored in Amazon S3 under a structured hierarchy (raw/, formatted/, enriched/, usage/).

- Transformation: Raw JSON data was converted to Parquet using PySpark, with schema enforcement, date normalization, flattening of nested arrays, and type corrections (e.g., boolean conversions).

- Combination: Using dbt on top of Amazon Athena, formatted data was joined into enriched datasets:
    - Linking Vélib station info with status data.
    - Merging Lime and Vélib into a unified dataset with provider identifiers.
    - Deduplication and functional filtering (only active bikes/stations).

- Indexation: Data was indexed into Elasticsearch with Spark jobs, where IDs were generated by concatenating station/bike IDs with timestamps, and geospatial fields were converted into [lat, lon] format for mapping.

- Machine Learning: A K-Means clustering algorithm (implemented in Spark) grouped bikes into usage clusters. To account for Vélib station bike counts, bike points were duplicated proportionally to the number of available units (a custom form of weighted clustering).

- Visualization: Kibana dashboards were built over two indexes (all_bike_data, kmeans_results) to display:
    - Distribution of bikes across Paris and suburbs.
    - Operator predominance in each cluster.
    - Real-time maps showing activity zones and competitive dynamics.


## Technical Deep Dive

To ensure scalability and maintainability, the project followed best practices in modern data engineering:
- **Airflow DAG Design**: The DAG was structured into modular tasks (ingestion, transformation, enrichment, ML, indexation) executed every 3 hours. Tasks exchanged data locations via XCom. A dedicated start_task operator ensured proper initialization.
- **Data Lake Organization**: Data was partitioned in S3 by layer (raw, formatted, enriched, usage), source (Velib, Lime), and timestamp (date/hour subfolders), following a clean naming convention.
- **Testing & Validation**:
    - Unit tests on ingestion scripts (e.g., test_lime_data_fetcher.py).
    - Schema validation in dbt tests ensuring uniqueness by [id, time] and absence of null values.
    - Manual verification of Parquet outputs using inspection scripts.
- Packages & Dependencies: apache-airflow, pyspark, dbt-athena-community, elasticsearch, boto3, pyarrow, s3fs.

Each tool integrated seamlessly via Airflow connections and environment variables.
Elastic Index Preparation: Pre-defined mappings were created to ensure proper indexing of geospatial fields, avoiding issues with auto-indexing.

![Airflow](/images/datalake/airflow_dag.png)




## Key Results

The final dashboards revealed interesting insights about the urban mobility market:
Market Distribution: Roughly 20,000 Vélib bikes and 10,000 Lime bikes are active in Île-de-France, with varying presence across neighborhoods.

- Usage Clusters: K-Means clustering highlighted dense activity zones, showing where each operator dominates and where gaps exist.
- Competitive Insights: Lime is absent in certain western zones, while in central areas it competes strongly with Vélib.
- Operational Metrics: By comparing stationary versus moving bikes, we estimated around 4,000 bikes are in active use at peak times.

These findings can help mobility operators refine their deployment strategies, competitive positioning, and service planning.

![Visu1](/images/datalake/visu1.png)


![Visu2](/images/datalake/visu2.png)

![Visu3](/images/datalake/visu3.png)



## Next Steps

This project successfully delivered a mini DataLake handling ingestion, transformation, enrichment, machine learning, and visualization.
It is a strong example of how technical data engineering can drive business intelligence. By combining APIs, AWS cloud services, PySpark transformations, dbt modeling, Elasticsearch indexing, and machine learning, I built a fully functional DataLake that transforms raw mobility data into strategic insights for the Île-de-France bike-sharing market.

Future enhancements could include:
- Transitioning from batch (3-hour DAG runs) to real-time streaming pipelines.
- Adding filtering capabilities by district/neighborhood for more granular insights.
- Performing flow analysis to identify commuting patterns across times of day.
- Leveraging additional Vélib datasets, such as station occupancy rates, to study complementarities between operators.
